{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "url = 'https://raw.githubusercontent.com/patriciof3/mentoria13_famaf_2024/main/df_reddit_mentoria.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabajo Práctico 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de un primer acercamiento, es probable que hayamos identificado algunos problemas en nuestra base de datos. El objetivo del presente práctico es solucionarlos definitivamente y dejar listo un dataset que nos permita trabajar de manera correcta con diferentes modelos de clasificación o clusterización. Para ello deberemos realizar una limpieza adecuada de la base, que descarte entradas nulas, spam, texto en otros idiomas y stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming y Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego lo que debe hacerse es un proceso de normalización de palabras, es decir, reducir la variabilidad que existe entre vocablos de la misma familia.\n",
    "\n",
    "Para ello existen en principio dos técnicas diferentes:\n",
    "\n",
    "- Stemming: Cortar las palabras para dejar solo su raíz\n",
    "- Lemmatización: Se utilizan librerías con funciones preentrenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Empecé con esto desde hace ya tiempo  años las razones son variadas ya que hay historial de enfermedades mentales en mi familia y pase por un momento duro yo sola en aquel entonces Pedi ayuda  años después y estuve medicada por un período muy corto de tiempo He sentido de todo desde las sensaciones más leves hasta las que tengo día con día En particular sufro de mareos y cuando estoy en la escuela tengo dificultad para respirar Mi punto es ustedes creen que haya alguien superado esto por completo Había aceptado vivir asi pero despues empecé a comparar mi vida con la de otras personas y quisiera al menos vivir sin tener esta mentalidad Hace no mucho tiempo logré controlarme Estaba a punto de explotar en una plaza comercial y simplemente no me lo permití Tuve una muy buena semana después de eso pero estar en calma es tan agotador  más porque nunca pensé que tuviera que esforzarme en ser feliz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tomamos una entrada y removemos caracteres especiales\n",
    "example = re.sub(r'[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ\\s]', '', df[\"content\"][130])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# remover stopwords\n",
    "\n",
    "stop_words = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.text.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "example_stopwords = preprocess_text(example)\n",
    "\n",
    "doc = nlp(example_stopwords)\n",
    "\n",
    "tokenized = tokens = [token.text for token in doc]\n",
    "\n",
    "\n",
    "# Stemming\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in tokenized]\n",
    "\n",
    "# Lematización\n",
    "\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "df_example = pd.DataFrame(\n",
    "    {'Original': tokenized,\n",
    "     'Stemming': stemmed_words,\n",
    "     'Lemmatization': lemmatized_words\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Stemming</th>\n",
       "      <th>Lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Empecé</td>\n",
       "      <td>empec</td>\n",
       "      <td>empezar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>razones</td>\n",
       "      <td>razon</td>\n",
       "      <td>razón</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>variadas</td>\n",
       "      <td>vari</td>\n",
       "      <td>variado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pase</td>\n",
       "      <td>pas</td>\n",
       "      <td>pasar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ayuda</td>\n",
       "      <td>ayud</td>\n",
       "      <td>ayudar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>estuve</td>\n",
       "      <td>estuv</td>\n",
       "      <td>estar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>medicada</td>\n",
       "      <td>medic</td>\n",
       "      <td>medicado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sensaciones</td>\n",
       "      <td>sensacion</td>\n",
       "      <td>sensación</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>leves</td>\n",
       "      <td>lev</td>\n",
       "      <td>leve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>creen</td>\n",
       "      <td>cre</td>\n",
       "      <td>creer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>empecé</td>\n",
       "      <td>empec</td>\n",
       "      <td>empezar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>personas</td>\n",
       "      <td>person</td>\n",
       "      <td>persona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>quisiera</td>\n",
       "      <td>quis</td>\n",
       "      <td>querer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>pensé</td>\n",
       "      <td>pens</td>\n",
       "      <td>pensar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>tuviera</td>\n",
       "      <td>tuv</td>\n",
       "      <td>tener</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original   Stemming Lemmatization\n",
       "0        Empecé      empec       empezar\n",
       "4       razones      razon         razón\n",
       "5      variadas       vari       variado\n",
       "10         pase        pas         pasar\n",
       "14        ayuda       ayud        ayudar\n",
       "17       estuve      estuv         estar\n",
       "18     medicada      medic      medicado\n",
       "23  sensaciones  sensacion     sensación\n",
       "24        leves        lev          leve\n",
       "32        creen        cre         creer\n",
       "38       empecé      empec       empezar\n",
       "41     personas     person       persona\n",
       "42     quisiera       quis        querer\n",
       "59        pensé       pens        pensar\n",
       "60      tuviera        tuv         tener"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_example[df_example.nunique(axis=1) == df_example.shape[1]]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos un df apropiadamente curado, podemos pasar a realizar la vectorización de nuestros datos. Bag of Words (BoW) y TF-IDF (Term Frequency-Inverse Document Frequency) son dos técnicas fundamentales utilizadas en el procesamiento de lenguaje natural (NLP) para la representación de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Representa el texto como una bolsa de sus palabras, sin tener en cuenta el orden de las palabras ni su gramática. Cada documento se convierte en un vector de frecuencias de palabras, donde cada dimensión del vector corresponde a una palabra del vocabulario.\n",
    "\n",
    "### TF-IDF\n",
    "Pondera las palabras en un documento basado en su frecuencia en el documento (TF) y en la frecuencia inversa de documentos (IDF) que contienen esa palabra en el corpus. La idea es reducir la importancia de las palabras comunes y resaltar las palabras raras pero relevantes. Un valor alto de TF-IDF indica que el término es muy relevante para ese documento en particular y no es común en el corpus. Un valor bajo indica que el término no es muy relevante para ese documento específico, o es muy común en el corpus y, por lo tanto, no aporta mucha información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ansiedad</th>\n",
       "      <th>ansioso</th>\n",
       "      <th>arruinar</th>\n",
       "      <th>ataque</th>\n",
       "      <th>constante</th>\n",
       "      <th>depresión</th>\n",
       "      <th>deprimido</th>\n",
       "      <th>dormir</th>\n",
       "      <th>energía</th>\n",
       "      <th>frecuentemente</th>\n",
       "      <th>pánico</th>\n",
       "      <th>sentir</th>\n",
       "      <th>tener</th>\n",
       "      <th>tristeza</th>\n",
       "      <th>vida</th>\n",
       "      <th>últimamente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ansiedad  ansioso  arruinar  ataque  constante  depresión  deprimido  \\\n",
       "0         0        1         0       0          0          0          0   \n",
       "1         1        0         0       0          0          0          0   \n",
       "2         0        0         0       0          0          0          1   \n",
       "3         0        0         1       0          0          1          0   \n",
       "4         0        0         0       1          0          0          0   \n",
       "5         0        0         0       0          1          0          0   \n",
       "\n",
       "   dormir  energía  frecuentemente  pánico  sentir  tener  tristeza  vida  \\\n",
       "0       0        0               0       0       1      0         0     0   \n",
       "1       1        0               0       0       0      0         0     0   \n",
       "2       0        1               0       0       1      0         0     0   \n",
       "3       0        0               0       0       0      0         0     1   \n",
       "4       0        0               1       1       0      1         0     0   \n",
       "5       0        0               0       0       0      0         1     0   \n",
       "\n",
       "   últimamente  \n",
       "0            1  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ansiedad</th>\n",
       "      <th>ansioso</th>\n",
       "      <th>arruinar</th>\n",
       "      <th>ataque</th>\n",
       "      <th>constante</th>\n",
       "      <th>depresión</th>\n",
       "      <th>deprimido</th>\n",
       "      <th>dormir</th>\n",
       "      <th>energía</th>\n",
       "      <th>frecuentemente</th>\n",
       "      <th>pánico</th>\n",
       "      <th>sentir</th>\n",
       "      <th>tener</th>\n",
       "      <th>tristeza</th>\n",
       "      <th>vida</th>\n",
       "      <th>últimamente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ansiedad  ansioso  arruinar  ataque  constante  depresión  deprimido  \\\n",
       "0      0.00     0.61      0.00    0.00       0.00       0.00       0.00   \n",
       "1      0.71     0.00      0.00    0.00       0.00       0.00       0.00   \n",
       "2      0.00     0.00      0.00    0.00       0.00       0.00       0.61   \n",
       "3      0.00     0.00      0.58    0.00       0.00       0.58       0.00   \n",
       "4      0.00     0.00      0.00    0.50       0.00       0.00       0.00   \n",
       "5      0.00     0.00      0.00    0.00       0.71       0.00       0.00   \n",
       "\n",
       "   dormir  energía  frecuentemente  pánico  sentir  tener  tristeza  vida  \\\n",
       "0    0.00     0.00            0.00    0.00    0.50   0.00      0.00  0.00   \n",
       "1    0.71     0.00            0.00    0.00    0.00   0.00      0.00  0.00   \n",
       "2    0.00     0.61            0.00    0.00    0.50   0.00      0.00  0.00   \n",
       "3    0.00     0.00            0.00    0.00    0.00   0.00      0.00  0.58   \n",
       "4    0.00     0.00            0.50    0.50    0.00   0.50      0.00  0.00   \n",
       "5    0.00     0.00            0.00    0.00    0.00   0.00      0.71  0.00   \n",
       "\n",
       "   últimamente  \n",
       "0         0.61  \n",
       "1         0.00  \n",
       "2         0.00  \n",
       "3         0.00  \n",
       "4         0.00  \n",
       "5         0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "# Crear DataFrame dummy con comentarios de Reddit\n",
    "data = {\n",
    "    'content': [\n",
    "        'Me siento muy ansioso últimamente',\n",
    "        'No puedo dormir debido a la ansiedad',\n",
    "        'Me siento deprimido y sin energía',\n",
    "        'La depresión está arruinando mi vida',\n",
    "        'Estoy teniendo ataques de pánico frecuentemente',\n",
    "        'La tristeza es constante y no sé qué hacer'\n",
    "    ],\n",
    "    'subreddit': ['Ansiedad', 'Ansiedad', 'Depresión', 'Depresión', 'Ansiedad', 'Depresión']\n",
    "}\n",
    "\n",
    "df_example = pd.DataFrame(data)\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Aplicar la función de preprocesamiento a la columna de contents\n",
    "df_example['content'] = df_example['content'].apply(preprocess)\n",
    "\n",
    "# Implementación de Bag of Words\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(df_example['content'])\n",
    "\n",
    "# Convertir a DataFrame para mejor visualización\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "\n",
    "# Implementación de TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df_example['content'])\n",
    "\n",
    "# Convertir a DataFrame para mejor visualización\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "\n",
    "# Mostrar los DataFrames\n",
    "print(\"Bag of Words (BoW):\")\n",
    "display(df_bow)\n",
    "\n",
    "print(\"\\nTF-IDF:\")\n",
    "display(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado de Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patricio\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, lowercase=True)\n",
    "X = vectorizer.fit_transform(df_example['content'])\n",
    "\n",
    "\n",
    "num_topics = 5 # select number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "\n",
    "df_example['topic_probabilities'] = lda.transform(X).tolist()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "prueba = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne', n_jobs=1)\n",
    "pyLDAvis.save_html(prueba, \"prueba.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
