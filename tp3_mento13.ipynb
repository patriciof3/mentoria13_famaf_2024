{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Entrenando un modelo de clasificación\n",
    "\n",
    "FECHA DE ENTREGA: 16/09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este trabajo práctico nos vamos a enfocar en el entrenamiento y evaluación de modelos de clasificación. La novedad es que vamos a incluir un set de datos nuevo. Ya está subida al repositorio una serie de posteos de r/espanol, un foro sin un carácter específico que nuclea conversaciones random en español. La idea es que entrenemos un modelo que clasifique los textos según su pertenencia a los tres subreddits que ahora forman parte de nuestros datos. \n",
    "\n",
    "Les dejo una lista de tareas para organizar el tp:\n",
    "\n",
    "1. Preparación de los Datos\n",
    "\n",
    "    Revisión del Preprocesamiento: Vienen probando distintas técnicas de preprocesado. Pueden trabajar en limpiar el código, modularizarlo y documentarlo apropiadamente. De esta forma va a ser más fácil implementar pequeñas variaciones a los datos para comparar rendimientos de clasificación y también aplicar los procesamientos ya desarrollados a los nuevos datos de r/espanol.\n",
    "\n",
    "    Si bien los diferentes modelos y sus hiperparámetros se llevan siempre la atención, la calidad del preprocesamiento va a impactar directamente en el rendimiento de los modelos de clasificación.\n",
    "\n",
    "    Vectorización: Esto también ya lo hemos incluido en los trabajos anteriores, ahora pueden comparar rendimientos entre una vectorización con **CountVectorizer** y una con **TfidfVectorizer**. Recuerden que los vectorizadores también tienen parámetros seteables, un ejemplo puede ser **ngram_range**  \n",
    "\n",
    "2. Entrenamiento de Modelos\n",
    "\n",
    "    Probar Diversos Modelos: Experimenten con al menos tres modelos de clasificación y utilicen técnicas como **GridSearchCV** para encontrar la mejor combinación de hiperparámetros para los modelos que elijan. Aunque el objetivo final es un modelo de clasificación que distinga entre los tres subreddits, si lo consideran útil también pueden comenzar entrenando uno sólo para los dos foros originales.\n",
    "\n",
    "3. Evaluación del Modelo\n",
    "\n",
    "    Métricas de Evaluación: Miden el rendimiento de los modelos utilizando métricas clave, tales como los informes de clasificación y las matrices de confusión.\n",
    "\n",
    "    Comparación de Resultados: Comparen el desempeño de los diferentes modelos y técnicas de vectorización. Analicen qué combinaciones ofrecieron mejores resultados y discutan las posibles razones.\n",
    "\n",
    "Una sugerencia que puede ser interesante en esta etapa es utilizar una **Pipeline** para organizar y simplificar el proceso de entrenamiento de modelos. Con esta forma de organizar el código podemos encadenar múltiples etapas de procesamiento y modelado en una única estructura, asegurando que las transformaciones se apliquen de manera consistente tanto en el entrenamiento como en la evaluación.\n",
    "\n",
    "4. Es importante ahora que puedan interpretar los resultados obtenidos. Por ejemplo: ¿qué significan los valores de accuracy o recall que obtuvieron para cada categoría? ¿Con alguna tuvieron peores resultados que con otra? ¿Existieron muchos datos mal clasificados entre ansiedad y depresión? ¿Cómo se comportó el modelo en relación a r/espanol?\n",
    "\n",
    "5. Una tarea interesante puede ser la exploración de los datos mal clasificados. Observan alguna regularidad entre los comentarios mal clasificados. Pregunta ejemplo: ¿qué palabras son más comunes entre los datos que son de r/ansiedad pero fueron clasificados por nuestro modelo como de r/depresion? ¿Se referían quizás a padecimientos similares? ¿Los comentarios de agradecimiento fueron difíciles de clasificar? \n",
    "\n",
    "¿Qué tipo de posteos de r/espanol nuestro modelo está clasificando como propios de r/ansiedad o r/Depresion?\n",
    "\n",
    "\n",
    "Si bien no es necesario que desarrollen esto ahora, creo que de cara al video final puede ser interesante que piensen en las siguientes preguntas\n",
    "\n",
    "¿Qué obstáculos encontraron a lo largo de los tres trabajos?\n",
    "¿Cómo creen que podría mejorarse el rendimiento obtenido en los diferentes modelos?\n",
    "¿Qué tipo de implementación creen que podría tener un trabajo como este?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook demo\n",
    "Acá abajo les dejo un preprocesado simple, seguido de una pipeline que prueba dos tipos de vectorizadores, y testea hiperparámetros para un solo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/patriciof3/mentoria13_famaf_2024/main/df_reddit_mentoria.csv\"\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_espanol = \"https://raw.githubusercontent.com/patriciof3/mentoria13_famaf_2024/main/df_espanol_posts.csv\"\n",
    "\n",
    "df_espanol = pd.read_csv(url_espanol)\n",
    "\n",
    "df_espanol = df_espanol.sample(n=1600, random_state=1) #sampleo de 600 casos para tener un número similar al trabajado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df, df_espanol], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Depresion    1619\n",
       "espanol      1600\n",
       "ansiedad     1528\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el modelo de Spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stop_words = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "def preprocess_text(texto):\n",
    "    texto = texto.drop_duplicates()\n",
    "    texto = texto.str.lower()\n",
    "    texto = texto[texto.str.len() >= 30]\n",
    "    texto = texto.apply(lambda x: re.sub(r'[^a-zA-Z\\sáéíóúüñÁÉÍÓÚÜÑ]', '', x))\n",
    "    texto = texto[~texto.str.startswith((\"http\", \"deleted\", \"removed\", \"nan\"))]\n",
    "    return texto\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.text.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def crear_label(label):\n",
    "    if label == \"ansiedad\":\n",
    "        return 1\n",
    "    elif label == \"Depresion\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Preprocesar el contenido\n",
    "df_merged['processed_content'] = preprocess_text(df_merged['content'])\n",
    "\n",
    "# Remover valores NaN\n",
    "df_merged.dropna(subset=['processed_content'], inplace=True)\n",
    "\n",
    "# Remover stopwords y lematizar\n",
    "df_merged['processed_content'] = df_merged['processed_content'].apply(remove_stopwords)\n",
    "df_merged['processed_content'] = df_merged['processed_content'].apply(lemmatize_text)\n",
    "\n",
    "# Eliminar duplicados en contenido procesado\n",
    "df_merged = df_merged.drop_duplicates(subset='processed_content', keep='first')\n",
    "\n",
    "# Convertir etiquetas a valores binarios\n",
    "df_merged['target'] = df_merged['subreddit'].apply(crear_label)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merged['processed_content'], df_merged['target'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Mejores parámetros encontrados por GridSearch:\n",
      "{'classifier__C': 1, 'vectorizer': TfidfVectorizer()}\n",
      "Precisión del mejor modelo: 0.7507\n",
      "Informe de Clasificación del mejor modelo:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.77       203\n",
      "           1       0.81      0.72      0.76       242\n",
      "           2       0.73      0.72      0.73       261\n",
      "\n",
      "    accuracy                           0.75       706\n",
      "   macro avg       0.75      0.76      0.75       706\n",
      "weighted avg       0.75      0.75      0.75       706\n",
      "\n",
      "Matriz de Confusión del mejor modelo:\n",
      "[[167   7  29]\n",
      " [ 26 174  42]\n",
      " [ 38  34 189]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Configurar el pipeline con los modelos y vectorizadores\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression()) # en este caso solo probaremos un modelo\n",
    "])\n",
    "\n",
    "# Definir los parámetros para la búsqueda en cuadrícula\n",
    "param_grid = {\n",
    "    'vectorizer': [CountVectorizer(), TfidfVectorizer()], # ejercicio con dos vectorizaciones diferentes\n",
    "    'classifier__C': [0.1, 1, 10] # ajuste de hiperparámetros para LogisticRegression()\n",
    "}\n",
    "\n",
    "# Configurar y ejecutar GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor combinación de hiperparámetros\n",
    "print(\"Mejores parámetros encontrados por GridSearch:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluación del mejor modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión\n",
    "precision = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión del mejor modelo: {precision:.4f}\")\n",
    "\n",
    "# Informe de clasificación\n",
    "print(\"Informe de Clasificación del mejor modelo:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de Confusión del mejor modelo:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisión de textos pertenecientes a r/espanol mal clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las predicciones y las verdaderas etiquetas\n",
    "results_df = pd.DataFrame({\n",
    "    'Original Text': X_test,\n",
    "    'True Label': y_test,\n",
    "    'Predicted Label': y_pred\n",
    "})\n",
    "\n",
    "# Filtrar los textos que deberían ser 0 pero fueron clasificados como 1 o 2\n",
    "wrong_classifications = results_df[(results_df['True Label'] == 0) & (results_df['Predicted Label'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso perteneciente a r/espanol clasificado como r/depresion:\n",
      "callado introvertido tiempo ir dañar forma q vuelto adicto sustacia alcohol tratar encajar mala influencia destrui valor seguir amigo constantemente deprimido\n"
     ]
    }
   ],
   "source": [
    "print(\"Caso perteneciente a r/espanol clasificado como r/depresion:\")\n",
    "print(wrong_classifications.loc[4201, \"Original Text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
